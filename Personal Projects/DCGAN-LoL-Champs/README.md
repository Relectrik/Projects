# Applying Deep Convolutional Generative Adversarial Networks to LoL Champs

The motivation for this project was to experiment with generative AI. I decided that Image Generation would be a good start, and I've used PyTorch in the past, so I wasn't completely new to the structure of the neural networks. I was, however, fascinated by the the mechanisms of this specific model. It utilizes a Discriminator Network to learn what the training images look like, to then evaluate the efficacy of the image generated by the Generator network. It iterates through this process slowly tweaking the Generator network's hyperparamaters to be able to better trick the discriminator.

The [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) and accompanying [paper](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) helped guide me through the process of building and training this neural network. Rather than utilizing the same dataset as the tutorial, I wanted to experiment with yet another one of Riot Game's titles, League of Legends, to see if the generator can replicate the art style of a LoL champion. A drawback of this, however, was the significantly lower training data size. However, I still wanted to utilize this dataset to see how the network would handle training on a small dataset. The images were set 120x120 pixels, and were taken from the LoL API through data dragon. For the purposes of the network, they were taken in as 64x64 images to stay consistent with the structure of the network.

Since I was new to the space of generative AI, I wanted to experiment with batch sizes and training epochs. I based the structure of the network on the one provided in the tutorial. 

### Disciminator:

```
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is ``(nc) x 64 x 64``
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. ``(ndf) x 32 x 32``
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. ``(ndf*2) x 16 x 16``
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. ``(ndf*4) x 8 x 8``
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. ``(ndf*8) x 4 x 4``
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid(),
        )
```
### Generator:
```
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. ``(ngf*8) x 4 x 4``
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. ``(ngf*4) x 8 x 8``
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. ``(ngf*2) x 16 x 16``
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. ``(ngf) x 32 x 32``
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh(),
            # state size. ``(nc) x 120 x 120``
        )
```

After experimenting with different epochs and batch sizes, here were my results:

### 100 Epoch 32 Batch Size
![100 Epoch 32 Batch Size](plots/100%20Epoch%20_%2032%20Batch%20Size%20Loss.png)
![100 Epoch 32 Batch Size](plots/100%20Epoch%20_%2032%20Batch%20Size%20Fake%20Images.png)

### 100 Epoch 128 Batch Size
![100 Epoch 128 Batch Size](plots/100%20Epoch%20_%20128%20Batch%20Size%20Loss.png)
![100 Epoch 128 Batch Size](plots/100%20Epoch%20_%20128%20Batch%20Size%20Fake%20Images.png)

### 250 Epoch 32 Batch Size
![250 Epoch 32 Batch Size](plots/250%20Epoch%20_%2032%20Batch%20Size%20Loss.png)
![250 Epoch 32 Batch Size](plots/250%20Epoch%20_%2032%20Batch%20Size%20Fake%20Images.png)

### 250 Epoch 128 Batch Size
![250 Epoch 128 Batch Size](plots/250%20Epoch%20_%20128%20Batch%20Size%20Loss.png)
![250 Epoch 128 Batch Size](plots/250%20Epoch%20_%20128%20Batch%20Size%20Fake%20Images.png)

### 400 Epoch 32 Batch Size
![400 Epoch 32 Batch Size](plots/400%20Epoch%20_%2032%20Batch%20Size%20Loss.png)
![400 Epoch 32 Batch Size](plots/400%20Epoch%20_%2032%20Batch%20Size%20Fake%20Images.png)

### 400 Epoch 128 Batch Size
![400 Epoch 128 Batch Size](plots/400%20Epoch%20_%20128%20Batch%20Size%20Loss.png)
![400 Epoch 128 Batch Size](plots/400%20Epoch%20_%20128%20Batch%20Size%20Fake%20Images.png)

### 500 Epoch 128 Batch Size
![500 Epoch 128 Batch Size](plots/500%20Epoch%20_%20128%20Batch%20Size%20Loss.png)
![500 Epoch 128 Batch Size](plots/500%20Epoch%20_%20128%20Batch%20Size%20Fake%20Images.png)

### 600 Epoch 128 Batch Size
![600 Epoch 128 Batch Size](plots/600%20Epoch%20_%20128%20Batch%20Size%20Loss.png)
![600 Epoch 128 Batch Size](plots/600%20Epoch%20_%20128%20Batch%20Size%20Fake%20Images.png)

### 160 Batch Size, 1000 Epoch
![160 Batch Size, 1000 Epoch](plots/1000%20Epoch%20_%20160%20Batch%20Size.png)
![160 Batch Size, 1000 Epoch](plots/1000%20Epoch%20_%2032%20Batch%20Size%20Fake%20Images.png)

## Brief Analysis
I decided to compare two batch sizes (32 and 128) to explore how different numbers of training epochs would affect the loss and fake images generated by these networks. I noticed that a lower batch size would mean that the network did not take as many epochs to reduce the loss and produce results closer to the image. We can see this clearly in the 100 epoch training examples. The 128 batch size only seems to produce shadows and dark colors associated with the art style of the champions, whereas the 32 batch size can already represent color since the network has done more iterations on that small batch of images.

However, with more training epochs, there is a clear difference in terms of clarity and the 32 batch size images look almost pixelated and lossy, whereas the 128 batch size images look blurry and undefined. We can also see from the 1000 epoch one, due to the limitation in training data, overtraining the network will lead to a pixelated and lossy mess.

I believe that the network would be able to better replicate and almost copy the art of these LoL champions, however I believe the network has also done a fantastic job at mimicking the art style as a whole, and would be able to (in conjunction with some other network) impart the stylistic tendencies (perhaps specific to color palettes) to another art piece. 

### Ethics of AI Art
Another point of this project is to dicuss the possible pitfalls when dealing with technology that is capable of generating art so similar to work done by original creators. One thing we can say for certain is that all currently AI-generated art has to be trained in some way. It is impossible for current AI to create **original** art. It requires significant training data to be able to properly replicate the art, and therefore all AI Art is inspired by artists through its training. 

So when vasts amount of original art by creators is used as training data for AI like DALL-E, how can we attribute any form of credit to the original artists responsible for giving the model its intricate training to be able to produce an image in the first place? It's an impossible question to answer, and an impractical solution would be to associate all AI Generated Art with a list of all the artists contributing to its training data. Perhaps more rigorous processes regarding consent will be required in the future if one would like to use art to train a model. Maybe a model could be created to compare art from an original creator and see an AI-generated one to see the similarity.